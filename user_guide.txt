
### USER GUIDE 

🛠 How to Run the Script
1. Open the Project
Open the project folder using Visual Studio Code (VS Code).

2. Activate the Virtual Environment
Run the following command in the terminal:

# in terminal
venv/Scripts/activate
3. Run the Script
Use this command:

# in terminal
python main.py
You don’t need to type the full command — after typing the first few letters, press the [TAB] key to auto-complete paths or file names. For example:


python ma[TAB]
will automatically complete to main.py.

⚠️ Possible Errors and How to Fix Them
1. Google Drive Folder Creation Fails
Reason: This usually happens due to an authorization issue.
Solution:

Delete the mycreds file in the project folder.

Then re-run the script to re-authorize access.

2. Page Contains Data but Requires Login
Sometimes the script may fail to find the required HTML element and stop with an error. This can happen if:

The page is asking for login and the script cannot access the data.

The script considers the URL invalid and skips it.

✅ Solution 1 (if the script simply stopped):
Just re-run the script. It will continue from where it left off.

✅ Solution 2 (if the script marked the URL as bad):
If you're sure the product exists on that URL, but it was marked as invalid (bad url), you’ll need to manually reset its status in the database.

Steps:

Open the product_data.db file using DB Browser for SQLite.

Go to "Open Database" and select the product_data.db file located in the project directory.

Click the "Execute SQL" tab.

Run the following SQL command to reset the product’s status:

UPDATE product_data SET scraped_status = 0 WHERE product_url = 'paste_the_product_url_here';
Save the changes and close the database.

Re-run the script.

⚠️ Note: This step is rarely needed, because in most cases, the script fails gracefully when it can't find the required element.

Deleting multiple URLs from the database (we need to delete the most recently added URLs).

First of all, in order to identify the incorrectly scraped products, we need to extract from the database the ones that have '404' as the value in the title_chn field. This is because when a URL does not exist on the site, is incorrect, or there is another error that prevents the page from being scraped, the value '404' is written to the title_chn field.

So, we find the products in the product_data table where title_chn = '404':

SELECT * FROM product_data WHERE title_chn = '404';

From the results, we then identify the point where incorrect scraping started. Let’s assume that in the previous correct scraping run, all the products were scraped properly, and only the ones removed from the platform or unavailable were marked as '404'. However, in the most recent run, all the products were incorrectly scraped.
We now need to find the first product with a '404' value that appeared after the last correct scraping. Once we find that product's ID, we delete all the products from the database with IDs greater than or equal to that value.

For example:

DELETE FROM product_data WHERE id >= 80;

(You determine this ID manually from the first query.)

Once these URLs are deleted, when the script is run again, it will insert the URLs into the database anew, and scraping will proceed.

General Note:
If the code crashes due to an error related to find_all (when it fails to find an HTML element), it usually means that the script was not able to fully render the HTML content via JavaScript. This is why the elements are not found.
